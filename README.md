# MCP Context Bridge ğŸŒ‰

**Cross-LLM context management system with intelligent summarization**

Share conversations seamlessly between ChatGPT, Claude, and Google Gemini with automatic smart compression.

## âœ¨ Features

- ğŸ”„ **Cross-LLM Context Sharing** - Continue conversations across different AI platforms
- ğŸ§  **Smart Summarization** - 99%+ compression while preserving key information
- ğŸš€ **Auto-Injection** - Context automatically loads when switching between LLMs
- ğŸ’¾ **MCP Protocol** - Standards-compliant Model Context Protocol implementation
- ğŸ“Š **Token Management** - Intelligent token counting and compression
- ğŸ” **Context Search** - Search through saved conversations (SQLite mode)

## ğŸ¯ Quick Start

### Prerequisites

- Node.js 16+
- Chrome/Edge browser
- npm or yarn

### 1. Server Setup

```bash
# Navigate to server directory
cd server

# Install dependencies
npm install

# Create .env file
cp .env.example .env

# Start server
npm run dev
```

Server runs on `http://localhost:3000`

### 2. Extension Installation

```bash
# Navigate to extension directory
cd extension

# Install dependencies
npm install

# Build extension
npm run build
```

**Install in Chrome:**
1. Open `chrome://extensions/`
2. Enable "Developer mode" (top-right)
3. Click "Load unpacked"
4. Select `extension/dist` folder
5. Extension icon appears in toolbar

## ğŸ“– Usage

### Save Conversations

1. **On ChatGPT/Gemini/Claude:**
   - Look for "ğŸ’¾ Save to MCP" button (bottom-right)
   - Click to save current conversation
   - Green notification confirms save

2. **Via Extension Popup:**
   - Click extension icon in toolbar
   - Click "ğŸ’¾ Save Conversation" button

### Cross-LLM Context Transfer

Context **automatically injects** when switching LLMs:

```
ChatGPT (save) â†’ Gemini (auto-loads context)
                    â†“
            Smart Summarization
            (99% compression)
```

**Example:**
1. Have conversation on ChatGPT
2. Click "Save to MCP"
3. Open Gemini â†’ Context auto-fills in textarea!
4. Continue the same conversation on Gemini

### Smart Summarization

The system automatically:
- **Detects LLM switches** (ChatGPT â†’ Gemini)
- **Compresses large contexts** (>4000 tokens)
- **Preserves key information:**
  - Important decisions
  - User preferences
  - Recent conversations (last 3)
  - Code snippets (last 2)
  - TODO items

**Compression Stats:**
- Typical: 40,000 tokens â†’ 323 tokens (99.2% reduction)
- Target: 2000-3000 tokens per summary

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChatGPT/Gemini â”‚
â”‚     Claude      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Content Scripts
         â”‚ (Extract & Inject)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Chrome Extension â”‚
â”‚ Service Worker  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP/MCP
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Server    â”‚
â”‚  (Node.js)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚JSON â”‚   â”‚SQLiteâ”‚
â”‚Storeâ”‚   â”‚ (FTS)â”‚
â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜
```

### Components

1. **MCP Server** - Node.js backend managing context storage
2. **Chrome Extension** - Browser integration with content scripts
3. **Smart Summarizer** - Rule-based context compression
4. **Storage** - JSON (default) or SQLite (with full-text search)

## âš™ï¸ Configuration

### Server (.env)

```env
# Server Configuration
PORT=3000
STORAGE_TYPE=json

# JSON Storage (default)
JSON_STORAGE_PATH=./data/contexts.json

# SQLite Storage (optional)
SQLITE_DB_PATH=./data/mcp.db

# Session
SESSION_TIMEOUT=7200000
```

### Extension Settings

Click extension icon â†’ Configure:
- **Server URL:** Default `http://localhost:3000`
- **Auto-inject context:** âœ… Enabled
- **Show notifications:** âœ… Enabled

## ğŸ”§ API Reference

### MCP Tools

#### `get_smart_summary`
Get intelligently summarized context

```json
{
  "name": "get_smart_summary",
  "arguments": {
    "source_llm": "chatgpt",
    "target_llm": "gemini",
    "max_tokens": 2500
  }
}
```

**Response:**
```json
{
  "content": "...",
  "originalTokens": 40000,
  "summaryTokens": 323,
  "compressionRatio": 0.008,
  "summarized": true
}
```

#### `add_context`
Save new context

```json
{
  "name": "add_context",
  "arguments": {
    "content": "...",
    "entry_type": "summary",
    "source_llm": "chatgpt",
    "metadata": {}
  }
}
```

### REST Endpoints

- `GET /health` - Server health check
- `POST /contexts/add` - Add new context
- `GET /contexts/all` - Get all contexts
- `POST /contexts/search` - Search contexts (SQLite only)
- `GET /session/info` - Session information

## ğŸ“Š Storage Options

### JSON Storage (Default)

**Pros:**
- Zero configuration
- Works immediately
- Simple file-based storage

**Cons:**
- No search capability
- Linear scan for queries

### SQLite Storage

**Pros:**
- Full-text search (FTS5)
- Efficient queries
- Better for large datasets

**Setup:**
```env
STORAGE_TYPE=sqlite
SQLITE_DB_PATH=./data/mcp.db
```

Server auto-creates database on first run.

## ğŸ§ª Testing

### Manual Testing

1. **Save on ChatGPT:**
   - Open ChatGPT
   - Have a conversation
   - Click "Save to MCP"
   - Check console (F12): Should see success message

2. **Load on Gemini:**
   - Open Gemini (new tab)
   - Press F12 (console)
   - Should see: `[MCP Extension] Context injected successfully!`
   - Textarea pre-filled with context

3. **Verify Compression:**
   - Check extension popup
   - View token count increase
   - Notification shows compression ratio

### Server Health Check

```bash
curl http://localhost:3000/health
```

Expected response:
```json
{
  "status": "ok",
  "session_id": "...",
  "contexts": 36,
  "tokens": 43400,
  "storage": "json"
}
```

## ğŸ› Troubleshooting

### Extension Issues

**"Extension context invalidated"**
- Solution: Reload extension â†’ Refresh page

**"Textarea not found"**
- LLM page structure changed
- Check console for selector logs
- Update content script selectors

**"Server offline"**
- Check server is running: `npm run dev`
- Verify port 3000 is not in use
- Check extension settings (server URL)

### Server Issues

**Port already in use:**
```bash
# Change PORT in .env
PORT=3001
```

**TypeScript compilation errors:**
```bash
# Rebuild
cd server
npm run build
```

## ğŸ“ Development

### Project Structure

```
context-mcp/
â”œâ”€â”€ server/              # MCP Server
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ index.ts           # Entry point
â”‚   â”‚   â”œâ”€â”€ config.ts          # Configuration
â”‚   â”‚   â”œâ”€â”€ types.ts           # TypeScript types
â”‚   â”‚   â”œâ”€â”€ mcp/               # MCP protocol handlers
â”‚   â”‚   â”œâ”€â”€ services/          # Business logic (summarizer)
â”‚   â”‚   â”œâ”€â”€ storage/           # Storage implementations
â”‚   â”‚   â””â”€â”€ routes/            # REST API routes
â”‚   â””â”€â”€ data/            # Storage files
â”‚
â””â”€â”€ extension/           # Chrome Extension
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ background/        # Service worker
    â”‚   â”œâ”€â”€ content/           # Content scripts (ChatGPT, Gemini, Claude)
    â”‚   â”œâ”€â”€ popup/             # Extension popup UI
    â”‚   â”œâ”€â”€ api/               # MCP client
    â”‚   â””â”€â”€ types/             # TypeScript types
    â””â”€â”€ dist/            # Built extension
```

### Adding New LLM Support

1. **Create content script:**
   ```typescript
   // extension/src/content/newllm.ts
   (function() {
     const LLM_TYPE = 'newllm';
     // ... implement injectContext() and extractConversation()
   })();
   ```

2. **Update manifest.json:**
   ```json
   {
     "content_scripts": [
       {
         "matches": ["https://newllm.com/*"],
         "js": ["content-newllm.js"]
       }
     ]
   }
   ```

3. **Update webpack config:**
   ```js
   entry: {
     'content-newllm': './src/content/newllm.ts'
   }
   ```

4. **Rebuild:** `npm run build`

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

## ğŸ“œ License

MIT License - See LICENSE file

## ğŸ™ Acknowledgments

- MCP Protocol specification
- Claude, ChatGPT, and Gemini teams
- Open source community

## ğŸ“§ Support

- **Issues:** GitHub Issues
- **Discussions:** GitHub Discussions

---

**Made with â¤ï¸ for seamless cross-LLM conversations**
